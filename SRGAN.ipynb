{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from torch import Tensor\n",
    "import math\n",
    "import queue\n",
    "import threading\n",
    "from typing import Any, List\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 420\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "class Config:\n",
    "    only_test_y_channel = True\n",
    "    d_arch_name = \"discriminator\"\n",
    "    g_arch_name = \"srresnet_x4\"\n",
    "    in_channels = 3\n",
    "    out_channels = 3\n",
    "    channels = 64\n",
    "    num_rcb = 16\n",
    "    upscale_factor = 4\n",
    "    exp_name = \"SRGAN\"\n",
    "    train_gt_images_dir = f\"./data/ImageNet/SRGAN/train\"\n",
    "    test_gt_images_dir = f\"./data/test/Set5/GTmod12\"\n",
    "    test_lr_images_dir = f\"./data/test/Set5/LRbicx{upscale_factor}\"\n",
    "    gt_image_size = 96\n",
    "    batch_size = 16\n",
    "    num_workers = 4\n",
    "    epochs = 10\n",
    "    # Loss function weight\n",
    "    pixel_weight = 1.0\n",
    "    content_weight = 1.0\n",
    "    adversarial_weight = 0.001\n",
    "\n",
    "    # Feature extraction layer parameter configuration\n",
    "    feature_model_extractor_node = \"features.35\"\n",
    "    feature_model_normalize_mean = [0.485, 0.456, 0.406]\n",
    "    feature_model_normalize_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Optimizer parameter\n",
    "    model_lr = 1e-4\n",
    "    model_betas = (0.9, 0.999)\n",
    "    model_eps = 1e-8\n",
    "    model_weight_decay = 0.0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dynamically adjust the learning rate policy [100,000 | 200,000]\n",
    "    lr_scheduler_step_size = epochs//2\n",
    "    lr_scheduler_gamma = 0.1\n",
    "    #  How many iterations to print the training result\n",
    "    train_print_frequency = 100\n",
    "    valid_print_frequency = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
    "\n",
    "\n",
    "def image_resize(image: Any, scale_factor: float, antialiasing: bool = True) -> Any:\n",
    "    \"\"\"Implementation of `imresize` function in Matlab under Python language.\n",
    "    Args:\n",
    "        image: The input image.\n",
    "        scale_factor (float): Scale factor. The same scale applies for both height and width.\n",
    "        antialiasing (bool): Whether to apply antialiasing when down-sampling operations.\n",
    "            Caution: Bicubic down-sampling in `PIL` uses antialiasing by default. Default: ``True``.\n",
    "    Returns:\n",
    "        out_2 (np.ndarray): Output image with shape (c, h, w), [0, 1] range, w/o round\n",
    "    \"\"\"\n",
    "    squeeze_flag = False\n",
    "    if type(image).__module__ == np.__name__:  # numpy type\n",
    "        numpy_type = True\n",
    "        if image.ndim == 2:\n",
    "            image = image[:, :, None]\n",
    "            squeeze_flag = True\n",
    "        image = torch.from_numpy(image.transpose(2, 0, 1)).float()\n",
    "    else:\n",
    "        numpy_type = False\n",
    "        if image.ndim == 2:\n",
    "            image = image.unsqueeze(0)\n",
    "            squeeze_flag = True\n",
    "\n",
    "    in_c, in_h, in_w = image.size()\n",
    "    out_h, out_w = math.ceil(\n",
    "        in_h * scale_factor), math.ceil(in_w * scale_factor)\n",
    "    kernel_width = 4\n",
    "\n",
    "    # get weights and indices\n",
    "    weights_h, indices_h, sym_len_hs, sym_len_he = _calculate_weights_indices(\n",
    "        in_h, out_h, scale_factor, kernel_width,\n",
    "        antialiasing)\n",
    "    weights_w, indices_w, sym_len_ws, sym_len_we = _calculate_weights_indices(\n",
    "        in_w, out_w, scale_factor, kernel_width,\n",
    "        antialiasing)\n",
    "    # process H dimension\n",
    "    # symmetric copying\n",
    "    img_aug = torch.FloatTensor(in_c, in_h + sym_len_hs + sym_len_he, in_w)\n",
    "    img_aug.narrow(1, sym_len_hs, in_h).copy_(image)\n",
    "\n",
    "    sym_patch = image[:, :sym_len_hs, :]\n",
    "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
    "    img_aug.narrow(1, 0, sym_len_hs).copy_(sym_patch_inv)\n",
    "\n",
    "    sym_patch = image[:, -sym_len_he:, :]\n",
    "    inv_idx = torch.arange(sym_patch.size(1) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(1, inv_idx)\n",
    "    img_aug.narrow(1, sym_len_hs + in_h, sym_len_he).copy_(sym_patch_inv)\n",
    "\n",
    "    out_1 = torch.FloatTensor(in_c, out_h, in_w)\n",
    "    kernel_width = weights_h.size(1)\n",
    "    for i in range(out_h):\n",
    "        idx = int(indices_h[i][0])\n",
    "        for j in range(in_c):\n",
    "            out_1[j, i, :] = img_aug[\n",
    "                j, idx:idx + kernel_width,\n",
    "                :].transpose(0, 1).mv(weights_h[i])\n",
    "\n",
    "    # process W dimension\n",
    "    # symmetric copying\n",
    "    out_1_aug = torch.FloatTensor(in_c, out_h, in_w + sym_len_ws + sym_len_we)\n",
    "    out_1_aug.narrow(2, sym_len_ws, in_w).copy_(out_1)\n",
    "\n",
    "    sym_patch = out_1[:, :, :sym_len_ws]\n",
    "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
    "    out_1_aug.narrow(2, 0, sym_len_ws).copy_(sym_patch_inv)\n",
    "\n",
    "    sym_patch = out_1[:, :, -sym_len_we:]\n",
    "    inv_idx = torch.arange(sym_patch.size(2) - 1, -1, -1).long()\n",
    "    sym_patch_inv = sym_patch.index_select(2, inv_idx)\n",
    "    out_1_aug.narrow(2, sym_len_ws + in_w, sym_len_we).copy_(sym_patch_inv)\n",
    "\n",
    "    out_2 = torch.FloatTensor(in_c, out_h, out_w)\n",
    "    kernel_width = weights_w.size(1)\n",
    "    for i in range(out_w):\n",
    "        idx = int(indices_w[i][0])\n",
    "        for j in range(in_c):\n",
    "            out_2[j, :, i] = out_1_aug[\n",
    "                j, :, idx:idx +\n",
    "                kernel_width].mv(weights_w[i])\n",
    "\n",
    "    if squeeze_flag:\n",
    "        out_2 = out_2.squeeze(0)\n",
    "    if numpy_type:\n",
    "        out_2 = out_2.numpy()\n",
    "        if not squeeze_flag:\n",
    "            out_2 = out_2.transpose(1, 2, 0)\n",
    "\n",
    "    return out_2\n",
    "\n",
    "\n",
    "def center_crop(image: np.ndarray, image_size: int) -> np.ndarray:\n",
    "    \"\"\"Crop small image patches from one image center area.\n",
    "    Args:\n",
    "        image (np.ndarray): The input image for `OpenCV.imread`.\n",
    "        image_size (int): The size of the captured image area.\n",
    "    Returns:\n",
    "        patch_image (np.ndarray): Small patch image\n",
    "    \"\"\"\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    # Just need to find the top and left coordinates of the image\n",
    "    top = (image_height - image_size) // 2\n",
    "    left = (image_width - image_size) // 2\n",
    "\n",
    "    # Crop image patch\n",
    "    patch_image = image[top:top + image_size, left:left + image_size, ...]\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def random_crop(image: np.ndarray, image_size: int) -> np.ndarray:\n",
    "    \"\"\"Crop small image patches from one image.\n",
    "    Args:\n",
    "        image (np.ndarray): The input image for `OpenCV.imread`.\n",
    "        image_size (int): The size of the captured image area.\n",
    "    Returns:\n",
    "        patch_image (np.ndarray): Small patch image\n",
    "    \"\"\"\n",
    "    image_height, image_width = image.shape[:2]\n",
    "\n",
    "    # Just need to find the top and left coordinates of the image\n",
    "    top = random.randint(0, image_height - image_size)\n",
    "    left = random.randint(0, image_width - image_size)\n",
    "\n",
    "    # Crop image patch\n",
    "    patch_image = image[top:top + image_size, left:left + image_size, ...]\n",
    "\n",
    "    return patch_image\n",
    "\n",
    "\n",
    "def image_to_tensor(image: np.ndarray, range_norm: bool, half: bool) -> Tensor:\n",
    "    # Convert image data type to Tensor data type\n",
    "    tensor = torch.from_numpy(\n",
    "        np.ascontiguousarray(image)).permute(2, 0, 1).float()\n",
    "\n",
    "    # Scale the image data from [0, 1] to [-1, 1]\n",
    "    if range_norm:\n",
    "        tensor = tensor.mul(2.0).sub(1.0)\n",
    "\n",
    "    # Convert torch.float32 image data type to torch.half image data type\n",
    "    if half:\n",
    "        tensor = tensor.half()\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor: Tensor, range_norm: bool, half: bool) -> Any:\n",
    "    if range_norm:\n",
    "        tensor = tensor.add(1.0).div(2.0)\n",
    "    if half:\n",
    "        tensor = tensor.half()\n",
    "    image = tensor.squeeze(0).permute(1, 2, 0).mul(\n",
    "        255).clamp(0, 255).cpu().numpy().astype(\"uint8\")\n",
    "    return image\n",
    "\n",
    "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
    "\n",
    "\n",
    "def _cubic(x: Any) -> Any:\n",
    "    \"\"\"Implementation of `cubic` function in Matlab under Python language.\n",
    "    Args:\n",
    "        x: Element vector.\n",
    "    Returns:\n",
    "        Bicubic interpolation\n",
    "    \"\"\"\n",
    "    absx = torch.abs(x)\n",
    "    absx2 = absx ** 2\n",
    "    absx3 = absx ** 3\n",
    "    return (1.5 * absx3 - 2.5 * absx2 + 1) * ((absx <= 1).type_as(absx)) + (\n",
    "        -0.5 * absx3 + 2.5 * absx2 - 4 * absx + 2) * (\n",
    "        ((absx > 1) * (absx <= 2)).type_as(absx))\n",
    "\n",
    "# Code reference `https://github.com/xinntao/BasicSR/blob/master/basicsr/utils/matlab_functions.py`\n",
    "\n",
    "\n",
    "def _calculate_weights_indices(\n",
    "    in_length: int,\n",
    "        out_length: int,\n",
    "        scale: float,\n",
    "        kernel_width: int,\n",
    "        antialiasing: bool) -> List[np.ndarray | int]:\n",
    "    \"\"\"Implementation of `calculate_weights_indices` function in Matlab under Python language.\n",
    "    Args:\n",
    "        in_length (int): Input length.\n",
    "        out_length (int): Output length.\n",
    "        scale (float): Scale factor.\n",
    "        kernel_width (int): Kernel width.\n",
    "        antialiasing (bool): Whether to apply antialiasing when down-sampling operations.\n",
    "            Caution: Bicubic down-sampling in PIL uses antialiasing by default.\n",
    "    Returns:\n",
    "        weights, indices, sym_len_s, sym_len_e\n",
    "    \"\"\"\n",
    "    if (scale < 1) and antialiasing:\n",
    "        # Use a modified kernel (larger kernel width) to simultaneously\n",
    "        # interpolate and antialiasing\n",
    "        kernel_width = kernel_width / scale\n",
    "\n",
    "    # Output-space coordinates\n",
    "    x = torch.linspace(1, out_length, out_length)\n",
    "\n",
    "    # Input-space coordinates. Calculate the inverse mapping such that 0.5\n",
    "    # in output space maps to 0.5 in input space, and 0.5 + scale in output\n",
    "    # space maps to 1.5 in input space.\n",
    "    u = x / scale + 0.5 * (1 - 1 / scale)\n",
    "\n",
    "    # What is the left-most pixel that can be involved in the computation?\n",
    "    left = torch.floor(u - kernel_width / 2)\n",
    "\n",
    "    # What is the maximum number of pixels that can be involved in the\n",
    "    # computation?  Note: it's OK to use an extra pixel here; if the\n",
    "    # corresponding weights are all zero, it will be eliminated at the end\n",
    "    # of this function.\n",
    "    p = math.ceil(kernel_width) + 2\n",
    "\n",
    "    # The indices of the input pixels involved in computing the k-th output\n",
    "    # pixel are in row k of the indices matrix.\n",
    "    indices = left.view(out_length, 1).expand(out_length, p) + torch.linspace(0, p - 1, p).view(1, p).expand(\n",
    "        out_length, p)\n",
    "\n",
    "    # The weights used to compute the k-th output pixel are in row k of the\n",
    "    # weights matrix.\n",
    "    distance_to_center = u.view(out_length, 1).expand(out_length, p) - indices\n",
    "\n",
    "    # apply cubic kernel\n",
    "    if (scale < 1) and antialiasing:\n",
    "        weights = scale * _cubic(distance_to_center * scale)\n",
    "    else:\n",
    "        weights = _cubic(distance_to_center)\n",
    "\n",
    "    # Normalize the weights matrix so that each row sums to 1.\n",
    "    weights_sum = torch.sum(weights, 1).view(out_length, 1)\n",
    "    weights = weights / weights_sum.expand(out_length, p)\n",
    "\n",
    "    # If a column in weights is all zero, get rid of it. only consider the\n",
    "    # first and last column.\n",
    "    weights_zero_tmp = torch.sum((weights == 0), 0)\n",
    "    if not math.isclose(weights_zero_tmp[0], 0, rel_tol=1e-6):\n",
    "        indices = indices.narrow(1, 1, p - 2)\n",
    "        weights = weights.narrow(1, 1, p - 2)\n",
    "    if not math.isclose(weights_zero_tmp[-1], 0, rel_tol=1e-6):\n",
    "        indices = indices.narrow(1, 0, p - 2)\n",
    "        weights = weights.narrow(1, 0, p - 2)\n",
    "    weights = weights.contiguous()\n",
    "    indices = indices.contiguous()\n",
    "    sym_len_s = -indices.min() + 1\n",
    "    sym_len_e = indices.max() - in_length\n",
    "    indices = indices + sym_len_s - 1\n",
    "    return weights, indices, int(sym_len_s), int(sym_len_e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__module__': '__main__', 'only_test_y_channel': True, 'd_arch_name': 'discriminator', 'g_arch_name': 'srresnet_x4', 'in_channels': 3, 'out_channels': 3, 'channels': 64, 'num_rcb': 16, 'upscale_factor': 4, 'exp_name': 'SRGAN', 'train_gt_images_dir': './data/ImageNet/SRGAN/train', 'test_gt_images_dir': './data/test/Set5/GTmod12', 'test_lr_images_dir': './data/test/Set5/LRbicx4', 'gt_image_size': 96, 'batch_size': 16, 'num_workers': 4, 'epochs': 10, 'pixel_weight': 1.0, 'content_weight': 1.0, 'adversarial_weight': 0.001, 'feature_model_extractor_node': 'features.35', 'feature_model_normalize_mean': [0.485, 0.456, 0.406], 'feature_model_normalize_std': [0.229, 0.224, 0.225], 'model_lr': 0.0001, 'model_betas': (0.9, 0.999), 'model_eps': 1e-08, 'model_weight_decay': 0.0, 'device': device(type='cuda'), 'lr_scheduler_step_size': 5, 'lr_scheduler_gamma': 0.1, 'train_print_frequency': 100, 'valid_print_frequency': 1, '__dict__': <attribute '__dict__' of 'Config' objects>, '__weakref__': <attribute '__weakref__' of 'Config' objects>, '__doc__': None}\n"
     ]
    }
   ],
   "source": [
    "print(vars(Config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainValidImageDataset(Dataset):\n",
    "    \"\"\"Define training/valid dataset loading methods.\n",
    "    Args:\n",
    "        gt_image_dir (str): Train/Valid dataset address.\n",
    "        gt_image_size (int): Ground-truth resolution image size.\n",
    "        upscale_factor (int): Image up scale factor.\n",
    "        mode (str): Data set loading method, the training data set is for data enhancement, and the\n",
    "            verification dataset is not for data enhancement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            gt_image_dir: str,\n",
    "            gt_image_size: int,\n",
    "            upscale_factor: int,\n",
    "            mode: str,\n",
    "    ) -> None:\n",
    "        super(TrainValidImageDataset, self).__init__()\n",
    "        self.image_file_names = [\n",
    "            os.path.join(gt_image_dir, image_file_name) for image_file_name in\n",
    "            os.listdir(gt_image_dir)\n",
    "        ]\n",
    "        self.gt_image_size = gt_image_size\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.mode = mode\n",
    "\n",
    "    # this function is called by Torch DataLoader\n",
    "    def __getitem__(self, batch_index: int) -> dict[str:torch.Tensor]:\n",
    "        # Read a batch of image data\n",
    "        gt_image = cv2.imread(\n",
    "            self.image_file_names[batch_index]).astype(np.float32) / 255.\n",
    "\n",
    "        # Image processing operations\n",
    "        if self.mode == \"Train\":\n",
    "            gt_crop_image = random_crop(gt_image, self.gt_image_size)\n",
    "        elif self.mode == \"Valid\":\n",
    "            gt_crop_image = center_crop(gt_image, self.gt_image_size)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unsupported data processing model, please use `Train` or `Valid`.\")\n",
    "\n",
    "        lr_crop_image = image_resize(gt_crop_image, 1 / self.upscale_factor)\n",
    "\n",
    "        # BGR convert RGB\n",
    "        gt_crop_image = cv2.cvtColor(gt_crop_image, cv2.COLOR_BGR2RGB)\n",
    "        lr_crop_image = cv2.cvtColor(lr_crop_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert image data into Tensor stream format (PyTorch).\n",
    "        # Note: The range of input and output is between [0, 1]\n",
    "        gt_crop_tensor = image_to_tensor(gt_crop_image, False, False)\n",
    "        lr_crop_tensor = image_to_tensor(lr_crop_image, False, False)\n",
    "\n",
    "        return {\"gt\": gt_crop_tensor, \"lr\": lr_crop_tensor}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestImageDataset(Dataset):\n",
    "    \"\"\"Define Test dataset loading methods.\n",
    "    Args:\n",
    "        test_gt_images_dir (str): ground truth image in test image\n",
    "        test_lr_images_dir (str): low-resolution image in test image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, test_gt_images_dir: str, test_lr_images_dir: str) -> None:\n",
    "        super(TestImageDataset, self).__init__()\n",
    "        # Get all image file names in folder\n",
    "        self.gt_image_file_names = [os.path.join(\n",
    "            test_gt_images_dir, x) for x in os.listdir(test_gt_images_dir)]\n",
    "        self.lr_image_file_names = [os.path.join(\n",
    "            test_lr_images_dir, x) for x in os.listdir(test_lr_images_dir)]\n",
    "\n",
    "    def __getitem__(self, batch_index: int) -> List[torch.Tensor]:\n",
    "        # Read a batch of image data\n",
    "        gt_image = cv2.imread(\n",
    "            self.gt_image_file_names[batch_index]).astype(np.float32) / 255.\n",
    "        lr_image = cv2.imread(\n",
    "            self.lr_image_file_names[batch_index]).astype(np.float32) / 255.\n",
    "\n",
    "        # BGR convert RGB\n",
    "        gt_image = cv2.cvtColor(gt_image, cv2.COLOR_BGR2RGB)\n",
    "        lr_image = cv2.cvtColor(lr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Convert image data into Tensor stream format (PyTorch).\n",
    "        # Note: The range of input and output is between [0, 1]\n",
    "        gt_tensor = image_to_tensor(gt_image, False, False)\n",
    "        lr_tensor = image_to_tensor(lr_image, False, False)\n",
    "\n",
    "        return {\"gt\": gt_tensor, \"lr\": lr_tensor}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.gt_image_file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUDAPrefetcher:\n",
    "    \"\"\"Use the CUDA side to accelerate data reading.\n",
    "    Args:\n",
    "        dataloader (DataLoader): Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
    "        device (torch.device): Specify running device.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataloader: DataLoader, device: torch.device):\n",
    "        self.batch_data = None\n",
    "        self.original_dataloader = dataloader\n",
    "        self.device = device\n",
    "\n",
    "        self.data = iter(dataloader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.batch_data = next(self.data)\n",
    "        except StopIteration:\n",
    "            self.batch_data = None\n",
    "            return None\n",
    "\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            for k, v in self.batch_data.items():\n",
    "                print(k)\n",
    "                if torch.is_tensor(v):\n",
    "                    self.batch_data[k] = self.batch_data[k].to(\n",
    "                        self.device, non_blocking=True)\n",
    "\n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        batch_data = self.batch_data\n",
    "        self.preload()\n",
    "        return batch_data\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = iter(self.original_dataloader)\n",
    "        self.preload()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.original_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(srgan_config: Config) -> List[CUDAPrefetcher]:\n",
    "    # Load train, test and valid datasets\n",
    "    train_datasets = TrainValidImageDataset(srgan_config.train_gt_images_dir,\n",
    "                                            srgan_config.gt_image_size,\n",
    "                                            srgan_config.upscale_factor,\n",
    "                                            \"Train\")\n",
    "    test_datasets = TestImageDataset(\n",
    "        srgan_config.test_gt_images_dir, srgan_config.test_lr_images_dir)\n",
    "\n",
    "    # Generator all dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_datasets,\n",
    "        batch_size=srgan_config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=srgan_config.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        persistent_workers=True)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_datasets,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        persistent_workers=True)\n",
    "\n",
    "    # Place all data on the preprocessing data loader\n",
    "    train_prefetcher = CUDAPrefetcher(train_dataloader, srgan_config.device)\n",
    "    print(\"\")\n",
    "    test_prefetcher = CUDAPrefetcher(test_dataloader, srgan_config.device)\n",
    "\n",
    "    return train_prefetcher, test_prefetcher\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Architecture\n",
    "![Discriminator Architecture](./imgs/Discriminator_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # input size. (3) x 96 x 96\n",
    "            # k3n64s1\n",
    "            nn.Conv2d(3, 64, (3, 3), (1, 1), (1, 1), bias=True),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # state size. (64) x 48 x 48\n",
    "            # k3n64s2\n",
    "            nn.Conv2d(64, 64, (3, 3), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            #k3n128s1\n",
    "            nn.Conv2d(64, 128, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            # state size. (128) x 24 x 24\n",
    "            #k3n128s2\n",
    "            nn.Conv2d(128, 128, (3, 3), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            #k3n256s1\n",
    "            nn.Conv2d(128, 256, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            # state size. (256) x 12 x 12\n",
    "            #k3n256s2\n",
    "            nn.Conv2d(256, 256, (3, 3), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            #k3n512s1\n",
    "            nn.Conv2d(256, 512, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            # state size. (512) x 6 x 6\n",
    "            #k3n512s2\n",
    "            nn.Conv2d(512, 512, (3, 3), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 6 * 6, 1024),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Linear(1024, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Input image size must equal 96\n",
    "        assert x.shape[2] == 96 and x.shape[3] == 96, \"Image shape must equal 96x96\"\n",
    "\n",
    "        out = self.features(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Architecture\n",
    "![Generator Architecture](./imgs/Generator_arch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class _ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, channels: int) -> None:\n",
    "        super(_ResidualConvBlock, self).__init__()\n",
    "        self.rcb = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "        out = self.rcb(x)\n",
    "        out = torch.add(out, identity)\n",
    "        return out\n",
    "\n",
    "\n",
    "class _UpsampleBlock(nn.Module):\n",
    "    def __init__(self, channels: int, upscale_factor: int) -> None:\n",
    "        super(_UpsampleBlock, self).__init__()\n",
    "        self.upsample_block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels * upscale_factor *\n",
    "                      upscale_factor, (3, 3), (1, 1), (1, 1)),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.upsample_block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SRResNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            channels: int,\n",
    "            num_rcb: int,\n",
    "            upscale_factor: int\n",
    "    ) -> None:\n",
    "        super(SRResNet, self).__init__()\n",
    "        # Low frequency information extraction layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, channels, (9, 9), (1, 1), (4, 4)),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "\n",
    "        # High frequency information extraction block\n",
    "        trunk = []\n",
    "        for _ in range(num_rcb):\n",
    "            trunk.append(_ResidualConvBlock(channels))\n",
    "        self.trunk = nn.Sequential(*trunk)\n",
    "\n",
    "        # High-frequency information linear fusion layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, (3, 3), (1, 1), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "        )\n",
    "\n",
    "        # zoom block\n",
    "        upsampling = []\n",
    "        if upscale_factor == 2 or upscale_factor == 4 or upscale_factor == 8:\n",
    "            for _ in range(int(math.log(upscale_factor, 2))):\n",
    "                upsampling.append(_UpsampleBlock(channels, 2))\n",
    "        elif upscale_factor == 3:\n",
    "            upsampling.append(_UpsampleBlock(channels, 3))\n",
    "        self.upsampling = nn.Sequential(*upsampling)\n",
    "\n",
    "        # reconstruction block\n",
    "        self.conv3 = nn.Conv2d(channels, out_channels, (9, 9), (1, 1), (4, 4))\n",
    "\n",
    "        # Initialize neural network weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "    # Support torch.script function\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.trunk(out1)\n",
    "        out2 = self.conv2(out)\n",
    "        out = torch.add(out1, out2)\n",
    "        out = self.upsampling(out)\n",
    "        out = self.conv3(out)\n",
    "        out = torch.clamp_(out, 0.0, 1.0)\n",
    "        return out\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                nn.init.constant_(module.weight, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt\n",
      "lr\n",
      "\n",
      "gt\n",
      "lr\n"
     ]
    }
   ],
   "source": [
    "train_prefetcher, test_prefetcher = load_dataset(Config())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
